{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our training set documents for our Naive Bayes classifier.\n",
    "\n",
    "**docs** contains the documents. **labels** are the classes - True=Pro Government, False=Anti Government\n",
    "\n",
    "**unknowns** are the documents we want to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [\"surplus good economy jobs\", \n",
    "        \"good government listens\",\n",
    "        \"best budget investments\",\n",
    "        \"corrupt highest levels\",\n",
    "        \"resign crooks\",\n",
    "        \"government good jobs friends\"]\n",
    "\n",
    "labels=[True, True, True, False, False, False]\n",
    "\n",
    "unknowns = [\"This corrupt corrupt government should resign\", \"Good economy is generating new jobs and a budget surplus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenise the document corpus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['surplus', 'good', 'economy', 'jobs'],\n",
       " ['good', 'government', 'listens'],\n",
       " ['best', 'budget', 'investments'],\n",
       " ['corrupt', 'highest', 'levels'],\n",
       " ['resign', 'crooks'],\n",
       " ['government', 'good', 'jobs', 'friends']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [docs[i].split() for i in range(0, len(docs))]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and generate a Vocabulary vector and also calculate word counts split by class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['surplus', 'good', 'economy', 'jobs', 'government', 'listens', 'best', 'budget', 'investments', 'corrupt', 'highest', 'levels', 'resign', 'crooks', 'friends']\n",
      "Counts(Pro Government): [1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "Counts(Anti Government): [0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "V = []\n",
    "VCntPro=[]\n",
    "VCntAnti=[]\n",
    "\n",
    "for d in range(0, len(words)):\n",
    "    for w in range(0, len(words[d])):\n",
    "        word = words[d][w]\n",
    "        if word in V:\n",
    "            ndx = V.index(word)\n",
    "            if labels[d]:\n",
    "                VCntPro[ndx] = VCntPro[ndx]+1\n",
    "            else:\n",
    "                VCntAnti[ndx] = VCntAnti[ndx]+1\n",
    "        else:\n",
    "            V.append(word)\n",
    "            if labels[d]:\n",
    "                VCntPro.append(1)\n",
    "                VCntAnti.append(0)\n",
    "            else:\n",
    "                VCntAnti.append(1)\n",
    "                VCntPro.append(0)\n",
    "        \n",
    "        \n",
    "print \"Vocabulary:\", V\n",
    "print \"Counts(Pro Government):\", VCntPro\n",
    "print \"Counts(Anti Government):\", VCntAnti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate term frequencies per corpus document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,\n",
       "         0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         1.,  0.],\n",
       "       [ 0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = np.zeros([len(words), len(V)])\n",
    "\n",
    "for d in range(0, len(words)):\n",
    "    for w in range(0, len(words[d])):\n",
    "        word = words[d][w]\n",
    "        ndx = V.index(word)\n",
    "        tf[d, ndx] = tf[d, ndx]+1\n",
    "        \n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate probabilities for all the terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term probability(Pro Government): [0.08, 0.12, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04]\n",
      "Term probability(Anti Government): [0.041666666666666664, 0.08333333333333333, 0.041666666666666664, 0.08333333333333333, 0.08333333333333333, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333]\n"
     ]
    }
   ],
   "source": [
    "PPro = [float(VCntPro[i]+1)/(sum(VCntPro)+len(V)) for i in range(0,len(V))]\n",
    "PAnti = [float(VCntAnti[i]+1)/(sum(VCntAnti)+len(V)) for i in range(0,len(V))]\n",
    "\n",
    "print \"Term probability(Pro Government):\", PPro\n",
    "print \"Term probability(Anti Government):\", PAnti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate global class probablities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "sumPro = sum(labels)\n",
    "sumAnti = len(labels)-sum(labels)\n",
    "\n",
    "PProG = float(sumPro)/(sumPro+sumAnti)\n",
    "PAntiG = float(sumAnti)/(sumPro+sumAnti)\n",
    "\n",
    "print PProG\n",
    "print PAntiG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to classify our unknown documents...first generate term vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "corrupt *\n",
      "corrupt *\n",
      "government *\n",
      "should\n",
      "resign *\n",
      "Good\n",
      "economy *\n",
      "is\n",
      "generating\n",
      "new\n",
      "jobs *\n",
      "and\n",
      "a\n",
      "budget *\n",
      "surplus *\n",
      "[['This', 'corrupt', 'corrupt', 'government', 'should', 'resign'], ['Good', 'economy', 'is', 'generating', 'new', 'jobs', 'and', 'a', 'budget', 'surplus']]\n",
      "[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  2.  0.  0.  1.  0.  0.]\n",
      " [ 1.  0.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "unknownWords = [unknowns[i].split() for i in range(0, len(unknowns))]\n",
    "\n",
    "termVecs = np.zeros([len(unknownWords), len(V)])\n",
    "for d in range(0, len(unknownWords)):\n",
    "    for w in range(0, len(unknownWords[d])):\n",
    "        word = unknownWords[d][w]\n",
    "        if word in V:\n",
    "            print word, \"*\" \n",
    "            ndx = V.index(word)\n",
    "            termVecs[d,ndx]+=1\n",
    "        else:\n",
    "            print word\n",
    "\n",
    "\n",
    "print unknownWords\n",
    "print termVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate probabilities using Bayes' algorithm for both Pro and Anti classes for the unknowns and compare... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Pro)= 0.000128 P(Anti)= 0.000578703703704  =>  False\n",
      "P(Pro)= 2.048e-05 P(Anti)= 3.01408179012e-06  =>  True\n",
      "Predictions= [ 0.  1.]\n"
     ]
    }
   ],
   "source": [
    "predictions = np.zeros([len(unknowns)])\n",
    "for d in range(0, len(termVecs)):\n",
    "    PProW = float(PProG)\n",
    "    PAntiW = float(PAntiG)\n",
    "    for ndx in range(0, len(termVecs[d])):\n",
    "        if termVecs[d,ndx]>0:\n",
    "            PProW = PProW * termVecs[d,ndx]*PPro[ndx]\n",
    "            PAntiW = PAntiW * termVecs[d,ndx]*PAnti[ndx]\n",
    "        #print V[ndx], \"pro=\", PProW, \", Anti=\", PAntiW\n",
    "    predictions[d] = PProW>PAntiW\n",
    "    print \"P(Pro)=\", PProW, \"P(Anti)=\", PAntiW, \" => \", PProW>PAntiW\n",
    "    \n",
    "print \"Predictions=\",predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "corrupt *\n",
      "corrupt *\n",
      "government *\n",
      "should\n",
      "resign *\n",
      "P(Pro)= 2.56e-06 P(Anti)= 2.4112654321e-05  =>  False\n",
      "Good\n",
      "economy *\n",
      "is\n",
      "generating\n",
      "new\n",
      "jobs *\n",
      "and\n",
      "a\n",
      "budget *\n",
      "surplus *\n",
      "P(Pro)= 2.048e-05 P(Anti)= 3.01408179012e-06  =>  True\n",
      "Predictions= [ 0.  1.]\n"
     ]
    }
   ],
   "source": [
    "predictions = np.zeros([len(unknowns)])\n",
    "for d in range(0, len(unknownWords)):\n",
    "    PProW = float(PProG)\n",
    "    PAntiW = float(PAntiG)\n",
    "    for w in range(0, len(unknownWords[d])):\n",
    "        word = unknownWords[d][w]\n",
    "        if word in V:\n",
    "            print word, \"*\" \n",
    "            ndx = V.index(word)\n",
    "            PProW = PProW * PPro[ndx]\n",
    "            PAntiW = PAntiW * PAnti[ndx]\n",
    "        else:\n",
    "            print word\n",
    "    predictions[d] = PProW>PAntiW\n",
    "    print \"P(Pro)=\", PProW, \"P(Anti)=\", PAntiW, \" => \", PProW>PAntiW\n",
    "    \n",
    "print \"Predictions=\",predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True]\n",
      "[[ 0.90402155  0.09597845]\n",
      " [ 0.12829111  0.87170889]]\n",
      "[[-0.10090208 -2.3436316 ]\n",
      " [-2.05345331 -0.13729975]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(tf, labels)\n",
    "\n",
    "print clf.predict(termVecs)\n",
    "print clf.predict_proba(termVecs)\n",
    "print clf.predict_log_proba(termVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
